{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Intelligence Python Program by Mason Howes\n",
        "\n",
        "Utilizes **Machine Learning**, **Natural Language Processing**, **Classification**, **Data Visualization** and the **Automation of an Intelligent Behavior**\n",
        "\n",
        "This program uses a dataset of Amazon.com product reviews, specifically [this dataset](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews). According to the dataset description,\n",
        "\n",
        "> This is a large-scale Amazon Reviews dataset collected in 2023. This dataset contains 48.19 million items, and 571.54 million reviews from 54.51 million users\n",
        "\n",
        "The data collected spans from May 1996 to September 2023. Attributions at bottom of program.\n",
        "\n",
        "**IMPORTANT**: To run this program, please download [the review dataset](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews) as well as this Jupyter Notebook. Place the .csv file and the .ipynb file in the same file location, and make sure the .csv file is named \"food_products.csv\".\n",
        "\n",
        "This program will both compute information about the most positive words and reviews, as well as visualize the data as it is processed in cells marked with **Visualization**.\n",
        "\n",
        "TL;DR - View the visualizations produced with Seed 416 (uncomment in the code to achieve these results)"
      ],
      "metadata": {
        "id": "n1Fafqkfbs-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "Loads modules"
      ],
      "metadata": {
        "id": "lr3nvLyxb4ZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm5rEvC7bhS5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import math\n",
        "import string\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "sns.set()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reads the dataset into the program"
      ],
      "metadata": {
        "id": "31v7ckTueqTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = pd.read_csv('food_products.csv')\n",
        "\n",
        "# Uncomment the seed if you want to achieve similar visualizations to those provided in the pictures above\n",
        "# np.random.seed(416)"
      ],
      "metadata": {
        "id": "xU_U-aqOetG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts neutral sentiment due to lack of information gained from analysis"
      ],
      "metadata": {
        "id": "io4V53_0eWXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = products[products['rating'] != 3].copy()"
      ],
      "metadata": {
        "id": "wNC94flAefcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**: Distribution of the number of reviews per rating (scale of 1-5)"
      ],
      "metadata": {
        "id": "wZbO1lWcft5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Number of Reviews Per Rating')\n",
        "sns.histplot(products['rating'])"
      ],
      "metadata": {
        "id": "IEdxiVjugAFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declares ratings of 4-5 to be considered Positive, and 1-2 to be considered Negative. In the \"Sentiment\" column, +1 is used to represent Positive, and -1 for Negative."
      ],
      "metadata": {
        "id": "2ouTHCTagOaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
      ],
      "metadata": {
        "id": "WDaoSyx7gOH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a \"Word Count\" vector for model analysis.\n",
        "\n",
        "Removing punctuation and then obtaining word counts for each review."
      ],
      "metadata": {
        "id": "NhWlMBMxgity"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to mass remove punctuation from reviews\n",
        "def remove_punctuation(text):\n",
        "    if type(text) is str:\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Makes the counts for each review\n",
        "vectorizer = CountVectorizer()\n",
        "count_matrix = vectorizer.fit_transform(products['review_clean'])\n",
        "\n",
        "# Maps unique words as features\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# DataFrame creation with count information\n",
        "product_data = pd.DataFrame(count_matrix.toarray(),\n",
        "        index=products.index,\n",
        "        columns=features)\n",
        "\n",
        "# Adds old columns to the new DataFrame\n",
        "product_data['sentiment'] = products['sentiment']\n",
        "product_data['review_clean'] = products['review_clean']\n",
        "product_data['summary'] = products['summary']"
      ],
      "metadata": {
        "id": "5vNdNYJ1hCVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splits data into Training, Validation and Test sets for model training.\n",
        "\n",
        "(80% Training, 10% Validation, 10% Test)"
      ],
      "metadata": {
        "id": "-_wcOQTWhloh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_and_validation_data = train_test_split(product_data, test_size=0.2, random_state=3)\n",
        "validation_data, test_data = train_test_split(test_and_validation_data, test_size=0.5, random_state=3)"
      ],
      "metadata": {
        "id": "e1tsodLFhuLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the majority class for all datapoints to keep tabs on model accuracy"
      ],
      "metadata": {
        "id": "T9ooutrdh7dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes most frequent label\n",
        "vals, counts = np.unique(train_data[\"sentiment\"], return_counts=True)\n",
        "index = np.argmax(counts)\n",
        "majority_label = vals[index]\n",
        "\n",
        "# Finds validation accuracy for majority class classifier\n",
        "correct = 0\n",
        "for y in validation_data[\"sentiment\"]:\n",
        "    if y == majority_label:\n",
        "        correct += 1\n",
        "\n",
        "majority_classifier_validation_accuracy = correct / len(validation_data)"
      ],
      "metadata": {
        "id": "Ng_SQwZjip-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains a sentiment classifier with logistic regression"
      ],
      "metadata": {
        "id": "itXIWGKGjJz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model = LogisticRegression(penalty='l2', C=1e23, random_state=1)\n",
        "sentiment_model.fit(train_data[features], train_data['sentiment'])"
      ],
      "metadata": {
        "id": "BkGpVuHljNkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finds the most positive and negative word in the sentiment model"
      ],
      "metadata": {
        "id": "HmsQxuacjWmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_negative_word = features[np.argmin(coefficients)]\n",
        "most_positive_word = features[np.argmax(coefficients)]\n",
        "print('Most Negative Word:', most_negative_word)\n",
        "print('Most Positive Word:', most_positive_word)"
      ],
      "metadata": {
        "id": "4WaFSlJPjbFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finds the most positive and negative review in the sentiment model"
      ],
      "metadata": {
        "id": "bNHGjjuQkCMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = sentiment_model.predict_proba(validation_data[features])\n",
        "\n",
        "most_positive = validation_data.iloc[np.argmax(predictions[:,1])]\n",
        "most_negative = validation_data.iloc[np.argmax(predictions[:,0])]\n",
        "\n",
        "most_positive_review = most_positive[\"review_clean\"]\n",
        "most_negative_review = most_negative[\"review_clean\"]\n",
        "\n",
        "print('Most Positive Review:')\n",
        "print(most_positive_review)\n",
        "print()\n",
        "print('Most Negative Review:')\n",
        "print(most_negative_review)"
      ],
      "metadata": {
        "id": "1IrlcAV-kHLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computes the validation accuracy of the sentiment model"
      ],
      "metadata": {
        "id": "XPepSIvhkMWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_true = validation_data[\"sentiment\"]\n",
        "sent_pred = sentiment_model.predict(validation_data[features])\n",
        "\n",
        "sentiment_model_validation_accuracy = accuracy_score(sent_true, sent_pred)"
      ],
      "metadata": {
        "id": "2nRLYbJUkOOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**: Creates a confusion matrix to measure the accuracy of the sentiment model"
      ],
      "metadata": {
        "id": "Y0a_cZaskUjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(tp, fp, fn, tn):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix using the values\n",
        "       tp - True Positive\n",
        "       fp - False Positive\n",
        "       fn - False Negative\n",
        "       tn - True Negative\n",
        "    \"\"\"\n",
        "    data = np.matrix([[tp, fp], [fn, tn]])\n",
        "\n",
        "    sns.heatmap(data,annot=True,xticklabels=['Actual Pos', 'Actual Neg']\n",
        "              ,yticklabels=['Pred. Pos', 'Pred. Neg'])\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "tn, fp, fn, tp = confusion_matrix(sent_true, sent_pred).ravel()\n",
        "plot_confusion_matrix(tp=tp, fp=fp, tn=tn, fn=fn)"
      ],
      "metadata": {
        "id": "rzjXtnxNkcyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One potential issue with the current program is the way that words are mapped. There are many more unique words (features) than there are reviews (observations).\n",
        "\n",
        "This portion of the program implements **L2 Regularization** to help avoid overfitting the data, with the goal of **increasing the accuracy**."
      ],
      "metadata": {
        "id": "EKTQQNO-lJ_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization Penalty Setups\n",
        "l2_penalties = [0.01, 1, 4, 10, 1e2, 1e3, 1e5]\n",
        "l2_penalty_names = [f'coefficients [L2={l2_penalty:.0e}]'\n",
        "                    for l2_penalty in l2_penalties]"
      ],
      "metadata": {
        "id": "0MTVZ4ZSl1Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds coefficients to the table for each model"
      ],
      "metadata": {
        "id": "cpORNvxJl43i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef_table = pd.DataFrame(columns=['word'] + l2_penalty_names)\n",
        "coef_table['word'] = features"
      ],
      "metadata": {
        "id": "bvIkbk0WmJsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sets up empty list to store accuracies"
      ],
      "metadata": {
        "id": "mlzQWLqdmKKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_data = []"
      ],
      "metadata": {
        "id": "2dIcZSCwmRVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains L2 model"
      ],
      "metadata": {
        "id": "k5oRguXjmWTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for l2_penalty, l2_penalty_column_name in zip(l2_penalties, l2_penalty_names):\n",
        "    lr_model = LogisticRegression(penalty='l2', C=1/l2_penalty, fit_intercept=False, random_state=1)\n",
        "\n",
        "    lr_model.fit(train_data[features], train_data[\"sentiment\"])\n",
        "\n",
        "    # Saves coefficients\n",
        "    coef_table[l2_penalty_column_name] = lr_model.coef_[0]\n",
        "\n",
        "    # Calculates and saves the train and validation accuracies\n",
        "    train_accuracy = accuracy_score(train_data[\"sentiment\"],\n",
        "                                    lr_model.predict(train_data[features]))\n",
        "    validation_accuracy = accuracy_score(validation_data[\"sentiment\"],\n",
        "                                         lr_model.predict(validation_data[features]))\n",
        "    accuracy_data.append({\"l2_penalty\": l2_penalty,\n",
        "                          \"train_accuracy\": train_accuracy,\n",
        "                          \"validation_accuracy\": validation_accuracy})\n",
        ""
      ],
      "metadata": {
        "id": "glI0Mhbtmatf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finds 5 most Positive and 5 most Negetive words found in the L2 model"
      ],
      "metadata": {
        "id": "c5Q9435_mplg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = coef_table.nlargest(5, \"coefficients [L2=1e+00]\").iloc[:, 0]\n",
        "negative_words = coef_table.nsmallest(5, \"coefficients [L2=1e+00]\").iloc[:, 0]\n",
        "print(positive_words)\n",
        "print(negative_words)"
      ],
      "metadata": {
        "id": "flMPMNTdmtu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**: Observes effect the increase in L2 Regularization penalties has on the most positive and negative words"
      ],
      "metadata": {
        "id": "Hvj7RIP5mz9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
        "\n",
        "    # Plots coefficients given table w/ rows corresponding to words & columns to l2 penalty,\n",
        "    # list of + and - words, & list of 12 penalties\n",
        "    def get_cmap_value(cmap, i, total_words):\n",
        "\n",
        "        # Computes scale from i=0 to i=total_words - 1 for cmap\n",
        "        return cmap(0.8 * ((i + 1) / (total_words * 1.2) + 0.15))\n",
        "\n",
        "\n",
        "    def plot_coeffs_for_words(ax, words, cmap):\n",
        "\n",
        "        # Plots coeff paths for each word in words given axes & word list\n",
        "        words_df = table[table['word'].isin(words)]\n",
        "        words_df = words_df.reset_index(drop=True)\n",
        "\n",
        "        for i, row in words_df.iterrows():\n",
        "            color = get_cmap_value(cmap, i, len(words))\n",
        "            ax.plot(xx, row[row.index != 'word'], '-',\n",
        "                    label=row['word'], linewidth=4.0, color=color)\n",
        "\n",
        "    # Canvas creation\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
        "\n",
        "    # Set up the xs to plot and draw a line for y=0\n",
        "    xx = l2_penalty_list\n",
        "    ax.plot(xx, [0.] * len(xx), '--', linewidth=1, color='k')\n",
        "\n",
        "    # Plot the positive and negative coefficient paths\n",
        "    cmap_positive = plt.get_cmap('Reds')\n",
        "    cmap_negative = plt.get_cmap('Blues')\n",
        "    plot_coeffs_for_words(ax, positive_words, cmap_positive)\n",
        "    plot_coeffs_for_words(ax, negative_words, cmap_negative)\n",
        "\n",
        "    # Set up axis labels, scale, and legend\n",
        "    ax.legend(loc='best', ncol=2, prop={'size':16}, columnspacing=0.5 )\n",
        "    ax.set_title('Coefficient path')\n",
        "    ax.set_xlabel('L2 penalty ($\\lambda$)')\n",
        "    ax.set_ylabel('Coefficient value')\n",
        "    ax.set_xscale('log')\n",
        "\n",
        "\n",
        "make_coefficient_plot(coef_table, positive_words, negative_words, l2_penalty_list=l2_penalties)"
      ],
      "metadata": {
        "id": "S4WpH3ZTmvcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Dataset attribution\n",
        "\n",
        "2023 version\n",
        "\n",
        "Bridging Language and Items for Retrieval and Recommendation\n",
        "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, Julian McAuley\n",
        "arXiv"
      ],
      "metadata": {
        "id": "3h21FdMfdgXV"
      }
    }
  ]
}